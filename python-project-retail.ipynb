{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project scoping "},{"metadata":{},"cell_type":"markdown","source":"## Description"},{"metadata":{},"cell_type":"markdown","source":"The following notebook details our approach to extract valuable data from a retail dataset.\n\n1. We will first start by exploring and understanding the data at hand to extract valuable insight.\n\n2. We will profile and clean the data.\n\n3. We will experiment a few clustering algorithms to look for meaningful groups of customers."},{"metadata":{},"cell_type":"markdown","source":"## Data Collection"},{"metadata":{},"cell_type":"markdown","source":"We will base this analysis on the three given sheets : \n* Customer : Customer information including demographics (~ 127 KB) \n\n* Transaction : Transaction of customers (1,41 MB)\n\n* Product Hierarchy : Product information (588 B)"},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For data manipulation\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom pandas_profiling import ProfileReport\n\n# For interaction with OS\nimport os ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{},"cell_type":"markdown","source":"* Let's load the data into a Pandas dataframe :"},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIR = '/kaggle/input/retail-case-study-data/'\n\nPATH_CUSTOMER = INPUT_DIR + 'Customer.csv'\nPATH_TRANSACTION = INPUT_DIR + 'Transactions.csv'\nPATH_PROD_CAT = INPUT_DIR + 'prod_cat_info.csv'\n\ndf_customer = pd.read_csv(PATH_CUSTOMER)\ndf_transaction = pd.read_csv(PATH_TRANSACTION)\ndf_prod_cat = pd.read_csv(PATH_PROD_CAT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getDataframeInfo(df: pd.DataFrame, nb_row: int) -> pd.DataFrame:\n    print(df.shape)\n    print(df.columns)\n    return df.head(nb_row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customer "},{"metadata":{"trusted":true},"cell_type":"code","source":"getDataframeInfo(df_customer, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Transaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"getDataframeInfo(df_transaction, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getDataframeInfo(df_prod_cat, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's unify the column names to simplify further join operations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lowercase the column names and rename them when needed\ndef get_unified_table(df: pd.DataFrame, cols : dict = {}) -> pd.DataFrame:\n    \n    df = df.rename(columns=str.lower)\n    \n    if cols: df = df.rename(columns=cols)\n    return df\n    \n\ndf_customer = get_unified_table(df_customer, {'customer_id':'cust_id'})\ndf_transaction = get_unified_table(df_transaction, {'qty':'quantity'})\ndf_prod_cat = get_unified_table(df_prod_cat, {'prod_sub_cat_code':'prod_subcat_code'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_customer.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_transaction.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_cat.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create New Dataframe"},{"metadata":{},"cell_type":"markdown","source":"* We create a new dataframe **df** by joining the three input dataframes."},{"metadata":{},"cell_type":"markdown","source":"### Create prod_trans\n\nFirst, let's create **df_prod_trans** combining **df_transaction** and **df_prod_cat** data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_trans = df_transaction.merge(df_prod_cat,\n                         on=['prod_cat_code', 'prod_subcat_code'],\n                         how='left')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the shape of our data :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_trans.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 23 053 rows and 12 columns to explore."},{"metadata":{},"cell_type":"markdown","source":"* Let's see how many null values we have per column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_trans.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No null value found."},{"metadata":{},"cell_type":"markdown","source":"### Merge prod_trans with customer"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_prod_trans.merge(df_customer,\n                              on=['cust_id'],\n                              how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's compare the shape of our final dataframe vs the df_prod_trans"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prod_trans.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is the same number of rows in df_final and df_prod_trans, meaning that all the product transactions are present in the final table.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's check how many null values we have per column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's convert object dtype to dates (YYYY-mm-dd)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final[['dob', 'tran_date']].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['dob'] = pd.to_datetime(df_final[\"dob\"], format='%d-%m-%Y')\ndf_final['tran_date'] = pd.to_datetime(df_final[\"tran_date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final[['dob', 'tran_date']].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Missing values"},{"metadata":{},"cell_type":"markdown","source":"* Let's have a look at the percentage of missing values in each column to decide whether some columns should be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = df_final.isnull().sum() * 100 / len(df_final)\nmissing_values = pd.DataFrame({'col_name': df_final.columns,\n                              'percent_missing' : percent_missing})\nmissing_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Luckily, missing values only represents a very tiny part of the whole dataset. All the columns have less than 80% missing values, meaning that we don't have to drop any of them."},{"metadata":{},"cell_type":"markdown","source":"## Duplicated values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's drop the duplicate rows()"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_final.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{},"cell_type":"markdown","source":"* Let's take a look at the correlation between the numerical columns :"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_final.corr()\n\nplt.subplots(figsize=(20,15))\nax = sns.heatmap(corr,\n                 vmin=-1, vmax=1, center=0,\n                 cmap=sns.diverging_palette(10, 220, n=2000),\n                 linewidths=.5)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no column correlated perfectly correlated to another, so we don't drop any columns."},{"metadata":{},"cell_type":"markdown","source":"### Data inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's use the transaction_id as index"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.set_index('transaction_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the *Five number summary* :\n- the sample minimum (smallest observation)\n- the lower quartile or first quartile\n- the median (the middle value)\n- the upper quartile or third quartile\n- the sample maximum (largest observation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can already see from the description that some columns have negative values that are actually returns.\nLet's get rid of negative quantities, they should remove at the same time all the other negative values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret = df_final[(df_final['quantity']) > 0 ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's check that there are no negative values anymore"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret.set_index('transaction_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now that only actual purchases remain,let's check the columns for irregularities :"},{"metadata":{},"cell_type":"markdown","source":"#### 'quantity'\n\n- Let's start with ***'quantity'***."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(df_no_ret, y=\"quantity\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* At first glance, the boxplot doesn't show any outliers. Let's look at the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(df_no_ret, x=\"quantity\", range_x=(0,5))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 'rate'\n\n* Now let's look at ***'rate'***:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret['rate'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(df_no_ret, y=\"rate\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(df_no_ret, x=\"rate\", range_x=(0,1400))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret['rate'].value_counts(normalize=True) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tax"},{"metadata":{},"cell_type":"markdown","source":"* Let's have a look at ***'tax'***"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(df_no_ret, y=\"tax\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The boxplot show some outliers, above 700. Let's take a look at these data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret[df_no_ret['tax'] > 700].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Without the business context, we can't verify this, but we suppose these high tax are real ones, applied to some expensive prod categories as shown below : Electronics, Home and kitchen, some clothing... Let's keep these outliers."},{"metadata":{},"cell_type":"markdown","source":"#### total_amt"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(df_no_ret, y=\"total_amt\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### city_code"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(df_no_ret, y=\"city_code\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion & Insights"},{"metadata":{},"cell_type":"markdown","source":"- Let's check the most popular categorical variables by describing the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.loc[:,df_final.dtypes==\"object\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's look at the distribution for the following columns :\n    - gender\n    - store_type\n    - prod_cat\n    - prod_subcat\n    - city_code"},{"metadata":{},"cell_type":"markdown","source":"#### gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\n#sns.countplot(df_no_ret['gender'])\ndf_no_ret.groupby('gender')['cust_id'].count().sort_values(ascending=True).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Men represents the biggest part of the customers, although both gender are well represented."},{"metadata":{},"cell_type":"markdown","source":"#### store_type"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.countplot(df_no_ret['store_type'])\nplt.xlabel('Store Type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Half of the total purchases are made via eShop, while the remaining types are slightly equally distributed."},{"metadata":{},"cell_type":"markdown","source":"#### prod_cat"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(df_no_ret['prod_cat'])\nplt.xlabel('Product Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The top selling categories are in order : Books, Electronics and Home and kitchen. Then, followede by Footwear, Clothing and Bags."},{"metadata":{},"cell_type":"markdown","source":"#### prod_subcat"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\ndf_no_ret.groupby('prod_subcat')['prod_subcat'].count().plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('Product Subcategory')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### city_code"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\ndf_no_ret.groupby('city_code')['cust_id'].count().sort_values(ascending=True).plot(kind='barh')\nplt.xlabel('Count')\nplt.ylabel('City Code')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distrib_cols = ['prod_subcat_code', 'prod_cat_code', 'quantity', 'rate', 'tax', 'total_amt']\ndf_distrib = df_no_ret.loc[:,distrib_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_distrib.columns:\n    df_no_ret[col].plot(kind='hist')\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret = df_no_ret.set_index('transaction_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_ret.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile = ProfileReport(df_no_ret, title=\"Pandas Profiling Report\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.to_file(\"/kaggle/working/report.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}